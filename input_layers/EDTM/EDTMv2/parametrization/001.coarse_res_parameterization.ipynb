{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f169da3-e46a-4a5b-8b61-6044f3e42644",
   "metadata": {},
   "source": [
    "# Land relief parameterization in coarser resolution \n",
    "\n",
    "This scr{ip}t is used to generate the coarse resolution land relief parameters. The main difference from higher resolution is that the data can fit in RAM of the whole each continent. There the parameters can generate in one shot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b90ac-5a61-4ae4-b343-1a99643bb0ff",
   "metadata": {},
   "source": [
    "## Part 1: setup and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d16e8d4-25d8-4876-be76-976bef9121a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon,mapping,box\n",
    "from shapely import segmentize\n",
    "import time\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "from minio import Minio\n",
    "from eumap.misc import ttprint\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def find_equi_7_proj(epsg4326_bound,equi_7_proj):\n",
    "    # Extract coordinates from bounding box\n",
    "    xmin, ymin, xmax, ymax = epsg4326_bound\n",
    "\n",
    "    # Define coordinates for the polygon\n",
    "    polygon_coords = [(xmin, ymin), (xmax, ymin), (xmax, ymax),(xmin, ymax)]\n",
    "\n",
    "    # Create polygon\n",
    "    polygon = Polygon(polygon_coords)\n",
    "\n",
    "    # Create a GeoDataFrame with a single row representing the bounding box\n",
    "    gdf = gpd.GeoDataFrame(geometry=[Polygon(polygon_coords)])\n",
    "    # Set CRS for the GeoDataFrame\n",
    "    gdf.crs = 'EPSG:4326'\n",
    "    # Check the resulting GeoDataFrame\n",
    "    gdf.geometry = segmentize(gdf.geometry,max_segment_length=0.000277778)\n",
    "\n",
    "    return gdf.to_crs(equi_7_proj).geometry.bounds.values[0]\n",
    "\n",
    "def find_epsg4326_proj(equi7_bound,equi_7_proj):\n",
    "    # Extract coordinates from bounding box\n",
    "    xmin, ymin, xmax, ymax = equi7_bound\n",
    "\n",
    "    # Define coordinates for the polygon\n",
    "    polygon_coords = [(xmin, ymin), (xmax, ymin), (xmax, ymax),(xmin, ymax)]\n",
    "\n",
    "    # Create polygon\n",
    "    polygon = Polygon(polygon_coords)\n",
    "\n",
    "    # Create a GeoDataFrame with a single row representing the bounding box\n",
    "    gdf = gpd.GeoDataFrame(geometry=[Polygon(polygon_coords)])\n",
    "    # Set CRS for the GeoDataFrame\n",
    "    gdf.crs = equi_7_proj\n",
    "    gdf.geometry = segmentize(gdf.geometry,max_segment_length=30)\n",
    "    return gdf.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "\n",
    "equi7_ind={'AF':'+proj=aeqd +lat_0=8.5 +lon_0=21.5 +x_0=5621452.01998 +y_0=5990638.42298 +datum=WGS84 +units=m +no_defs',\n",
    "'AN':'+proj=aeqd +lat_0=-90 +lon_0=0 +x_0=3714266.97719 +y_0=3402016.50625 +datum=WGS84 +units=m +no_defs',\n",
    "'AS':'+proj=aeqd +lat_0=47 +lon_0=94 +x_0=4340913.84808 +y_0=4812712.92347 +datum=WGS84 +units=m +no_defs',\n",
    "'EU':'+proj=aeqd +lat_0=53 +lon_0=24 +x_0=5837287.81977 +y_0=2121415.69617 +datum=WGS84 +units=m +no_defs',\n",
    "'NA':'+proj=aeqd +lat_0=52 +lon_0=-97.5 +x_0=8264722.17686 +y_0=4867518.35323 +datum=WGS84 +units=m +no_defs',\n",
    "'OC':'+proj=aeqd +lat_0=-19.5 +lon_0=131.5 +x_0=6988408.5356 +y_0=7654884.53733 +datum=WGS84 +units=m +no_defs',\n",
    "'SA':'+proj=aeqd +lat_0=-14 +lon_0=-60.5 +x_0=7257179.23559 +y_0=5592024.44605 +datum=WGS84 +units=m +no_defs'}\n",
    "\n",
    "conts = ['AF',\n",
    "'AS',\n",
    "'EU',\n",
    "'NA',\n",
    "'OC',\n",
    "'SA']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15fde784-1977-4179-b93e-454fa8ccd3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>old_data_type</th>\n",
       "      <th>ori_min</th>\n",
       "      <th>ori_max</th>\n",
       "      <th>multiplier</th>\n",
       "      <th>new_data_type</th>\n",
       "      <th>final_min</th>\n",
       "      <th>final_max</th>\n",
       "      <th>no_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geomorphon</td>\n",
       "      <td>Int16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>Byte</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hillshade</td>\n",
       "      <td>Float32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28357.00000</td>\n",
       "      <td>1</td>\n",
       "      <td>UInt16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28357.000000</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slope.in.degree</td>\n",
       "      <td>Float32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.29000</td>\n",
       "      <td>100</td>\n",
       "      <td>UInt16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1529.000000</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ls.factor</td>\n",
       "      <td>Float32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.00000</td>\n",
       "      <td>1000</td>\n",
       "      <td>UInt16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13000.000000</td>\n",
       "      <td>65535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pro.curv</td>\n",
       "      <td>Float32</td>\n",
       "      <td>-8.210329</td>\n",
       "      <td>8.01173</td>\n",
       "      <td>1000</td>\n",
       "      <td>Int16</td>\n",
       "      <td>-8210.329056</td>\n",
       "      <td>8011.730194</td>\n",
       "      <td>32767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        parameters old_data_type   ori_min     ori_max   multiplier  \\\n",
       "0       geomorphon         Int16  1.000000     10.00000           1   \n",
       "1        hillshade       Float32  0.000000  28357.00000           1   \n",
       "2  slope.in.degree       Float32  0.000000     15.29000         100   \n",
       "3        ls.factor       Float32  0.000000     13.00000        1000   \n",
       "4         pro.curv       Float32 -8.210329      8.01173        1000   \n",
       "\n",
       "  new_data_type    final_min    final_max   no_data  \n",
       "0          Byte     1.000000     10.000000      255  \n",
       "1        UInt16     0.000000  28357.000000    65535  \n",
       "2        UInt16     0.000000   1529.000000    65535  \n",
       "3        UInt16     0.000000  13000.000000    65535  \n",
       "4         Int16 -8210.329056   8011.730194    32767  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale factors for each parameters\n",
    "p_table=pd.read_csv('scaling.csv')\n",
    "p_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2106e-965e-4bc9-8e84-9197e5d4fcf1",
   "metadata": {},
   "source": [
    "## Part 2: parameterization in the for-loop by continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e9f45-d294-44d2-b6c8-81ad04c41daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def worker_cont(cont,p_table):\n",
    "for cont in conts:\n",
    "    s3_config = {\n",
    "        'access_key': 's3_key',\n",
    "        'secret_access_key': 's3_secret_key',\n",
    "        'host': '{ip}',\n",
    "        'bucket': 'tmp-global-geomorpho'}\n",
    "    client = Minio(s3_config['host'], s3_config['access_key'], s3_config['secret_access_key'], secure=False) \n",
    "\n",
    "    for resolution in [960,480]:\n",
    "        url_cont=f'http://{ip}/tmp-global-geomorpho/v4/{cont}/tan.curv_edtm_m_{resolution}m_s_20000101_20221231_{cont.lower().replace(\"_\",\".\")}_equi7_v20241230.tif'\n",
    "        r = requests.head(url_cont)\n",
    "        if r.status_code == 200:\n",
    "            ttprint(f'{cont} has been process')\n",
    "            continue\n",
    "        equi_7_proj=equi7_ind[cont]\n",
    "        gpd_cont=gpd.read_file(f'Equi7Grid/src/equi7grid/grids/{cont}/PROJ/EQUI7_V14_{cont}_PROJ_ZONE.shp')\n",
    "        equi7_bound = np.array(gpd_cont.bounds)[0]\n",
    "        gdalwarp_bbox = ' '.join([str(i) for i in equi7_bound])\n",
    "        tmp_outdir=f'/tmp/tmp-global-geomorpho/{cont}'\n",
    "        os.makedirs(tmp_outdir,exist_ok=True)\n",
    "        gdal_cmd_reproject = f'gdalwarp  -t_srs \"{equi_7_proj}\" -tr {resolution} {resolution} -r average \\\n",
    "        -co TILED=YES -co BIGTIFF=YES -co COMPRESS=DEFLATE -co ZLEVEL=9 -co BLOCKXSIZE=1024 \\\n",
    "        -co BLOCKYSIZE=1024 -co NUM_THREADS=8 -co SPARSE_OK=TRUE -of GTiff -overwrite'\n",
    "        url_cont=f'http://{ip}/tmp-global-geomorpho/v4/{cont}/dtm_edtm_m_{resolution}m_s_20000101_20221231_{cont.lower().replace(\"_\",\".\")}_equi7_v20241230.tif'\n",
    "        r = requests.head(url_cont)\n",
    "        if r.status_code == 200:\n",
    "            ttprint(f'{cont} dtm is on S3')\n",
    "        else:\n",
    "\n",
    "            out_ori_file=f'dtm_edtm_m_{resolution}m_s_20000101_20221231_{cont.lower().replace(\"_\",\".\")}_equi7_v20241230.tif'\n",
    "            rn_file = f'{tmp_outdir}/{out_ori_file}'\n",
    "            filepath = '/vsicurl/{ip}/global/edtm/legendtm_rf_30m_m_s_20000101_20231231_go_epsg.4326_v20250130.tif'\n",
    "\n",
    "            os.system(f'{gdal_cmd_reproject} -te {gdalwarp_bbox} {filepath} {tmp_outdir}/scaled_dtm_tmp.tif')\n",
    "            os.system(f'gdal_calc.py --overwrite -A {tmp_outdir}/scaled_dtm_tmp.tif \\\n",
    "                    --outfile={rn_file} --calc=\"A * 0.1\" \\\n",
    "                    --type=Float32 --co=\"COMPRESS=DEFLATE\" --co=\"BLOCKXSIZE=2048\" --co=\"BLOCKYSIZE=2048\"')\n",
    "            s3_path = f\"v4/{cont}/{out_ori_file}\"\n",
    "            client.fput_object(s3_config['bucket'], s3_path, rn_file)\n",
    "            ttprint(f'{ip}/tmp-global-geomorpho/{s3_path} on S3')\n",
    "        \n",
    "        gdal_cmd = f'gdalwarp  \\\n",
    "        -co TILED=YES -co BIGTIFF=YES -co COMPRESS=DEFLATE -co ZLEVEL=9 -co BLOCKXSIZE=1024 \\\n",
    "        -co BLOCKYSIZE=1024 -co NUM_THREADS=8 -co SPARSE_OK=TRUE -of GTiff -overwrite'\n",
    "        input_file = f'{tmp_outdir}/dtm_cont.tif'\n",
    "        os.system(f'{gdal_cmd} \\\n",
    "                  /vsicurl/http://{ip}/tmp-global-geomorpho/v4/{cont}/dtm_edtm_m_{resolution}m_s_20000101_20221231_{cont.lower().replace(\"_\",\".\")}_equi7_v20241230.tif \\\n",
    "                  {input_file}')\n",
    "\n",
    "        import whitebox_workflows\n",
    "        from whitebox_workflows import download_sample_data, show, WbEnvironment\n",
    "\n",
    "        # Test your license by setting up the WbW environment\n",
    "        wbe = whitebox_workflows.WbEnvironment()\n",
    "        start_time = time.time()\n",
    "        #tmp_dtm_file=f'vsicrul/http://{ip}/tmp-global-geomorpho/{s3_path}'\n",
    "        # Reading raster data\n",
    "        dtm = wbe.read_raster(input_file)\n",
    "        ttprint(\"read_raster--- %s seconds ---\" % (time.time() - start_time))\n",
    "        outdir=f'/mnt/{server_name}/tmp-global-geomorpho/{cont}'\n",
    "        #outdir=f'/mnt/landmark/tmp-global-geomorpho/{cont}'\n",
    "        os.makedirs(outdir,exist_ok=True)\n",
    "        file_list=[]\n",
    "\n",
    "\n",
    "        global_landmask_file='http://{ip}/global/dsm.landmask_ensemble_m_30m_s_20000101_20221231_go_epsg.4326_v4.1.tif'\n",
    "        #tmp_ori_landmask_file = f'{tmp_outdir}/tmp_landmask.tif'\n",
    "        tmp_landmask_file = f'{tmp_outdir}/landmask.tif'\n",
    "        #os.system(f'{gdal_cmd_reproject} -r min {global_landmask_file} {tmp_landmask_file}')\n",
    "        os.system(f'{gdal_cmd_reproject} -te {gdalwarp_bbox} -r min {global_landmask_file} {tmp_landmask_file}')\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Reading raster data\n",
    "        ttprint(f\"{cont} read_raster--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # geomorphon\n",
    "        tmp_geomorphon_file=input_file.replace('dtm','geomorphon')\n",
    "        scale=p_table[p_table['parameters']=='geomorphon'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        geomorphon=wbe.geomorphons(dtm, search_distance=3, \n",
    "                                  output_forms=True, analyze_residuals=False)\n",
    "        wbe.write_raster(geomorphon*scale, tmp_geomorphon_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate geomporphon--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_geomorphon_file)\n",
    "\n",
    "\n",
    "        # fill depression for hydrological analysis\n",
    "        tmp_flled_dtm_file=input_file.replace('dtm','nodepress.dtm')\n",
    "        scale=p_table[p_table['parameters']=='nodepress.dtm'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        dtm_no_deps = wbe.breach_depressions_least_cost(dtm, fill_deps=True, flat_increment=0.001)\n",
    "        wbe.write_raster(dtm_no_deps*scale, tmp_flled_dtm_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} fill depressions--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_flled_dtm_file)\n",
    "\n",
    "\n",
    "        # slope for hydrology\n",
    "        tmp_slope_file=input_file.replace('dtm','slope.in.degree')\n",
    "        scale=p_table[p_table['parameters']=='slope.in.degree'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        slope_in_degree = wbe.slope(dtm)\n",
    "        wbe.write_raster(slope_in_degree*scale, tmp_slope_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate slope--- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_slope_file)\n",
    "\n",
    "        # SCA\n",
    "        tmp_sca_file=input_file.replace('dtm','spec.catch')\n",
    "        scale=p_table[p_table['parameters']=='spec.catch'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        sca = wbe.qin_flow_accumulation(dtm_no_deps, out_type='sca', log_transform=True)\n",
    "        wbe.write_raster(sca*scale, tmp_sca_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} specific catchment area--- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_sca_file)\n",
    "\n",
    "        # ls factor\n",
    "        tmp_lsfactor_file=input_file.replace('dtm','ls.factor')\n",
    "        start_time = time.time()\n",
    "        scale=p_table[p_table['parameters']=='ls.factor'].multiplier.iloc[0]\n",
    "\n",
    "        ls_factor=wbe.sediment_transport_index(sca, slope_in_degree, sca_exponent=0.4, slope_exponent=1.3)\n",
    "        wbe.write_raster(ls_factor*scale, tmp_lsfactor_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate ls factor--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_lsfactor_file)\n",
    "\n",
    "        #twi\n",
    "        start_time = time.time()\n",
    "        tmp_twi_file=input_file.replace('dtm','twi')\n",
    "        scale=p_table[p_table['parameters']=='twi'].multiplier.iloc[0]\n",
    "\n",
    "        twi = wbe.wetness_index(specific_catchment_area=sca, slope=slope_in_degree)\n",
    "        #twi_filled = wbe.fill_missing_data(twi, exclude_edge_nodata=True)\n",
    "        ttprint(f\"{cont} topographic wetness index--- %s seconds ---\" % (time.time() - start_time))\n",
    "        wbe.write_raster(twi*scale, tmp_twi_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        file_list.append(tmp_twi_file)\n",
    "\n",
    "        # Reading raster data\n",
    "        start_time = time.time()\n",
    "        # Reading raster data\n",
    "        ttprint(f\"{cont} read local surface raster--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        # diff from mean elev\n",
    "        start_time = time.time()\n",
    "        tmp_dfme_file=input_file.replace('dtm','dfme')\n",
    "        scale=p_table[p_table['parameters']=='dfme'].multiplier.iloc[0]\n",
    "\n",
    "        dfme=wbe.difference_from_mean_elevation(\n",
    "            dtm, \n",
    "            filter_size_x=3, \n",
    "            filter_size_y=3)\n",
    "\n",
    "        wbe.write_raster(dfme*scale, tmp_dfme_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} diff from mean elev--- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_dfme_file)\n",
    "\n",
    "        #Spherical Std Dev Of Normals\n",
    "        start_time = time.time()\n",
    "        tmp_ssdon_file=input_file.replace('dtm','ssdon')\n",
    "        scale=p_table[p_table['parameters']=='ssdon'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        ssdon=wbe.spherical_std_dev_of_normals(\n",
    "            dtm, \n",
    "            filter_size=3 \n",
    "        )\n",
    "\n",
    "        wbe.write_raster(ssdon*scale, tmp_ssdon_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} spherical std dev of normals--- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_ssdon_file)\n",
    "\n",
    "\n",
    "        # Hillshade\n",
    "        tmp_hillshade_file=input_file.replace('dtm','hillshade')\n",
    "        scale=p_table[p_table['parameters']=='hillshade'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        hs = wbe.multidirectional_hillshade(dtm)\n",
    "        wbe.write_raster(hs*scale, tmp_hillshade_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate hillshade--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_hillshade_file)\n",
    "\n",
    "        # Minic\n",
    "        tmp_minic_file=input_file.replace('dtm','minic')\n",
    "        scale=p_table[p_table['parameters']=='minic'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        minic = wbe.minimal_curvature(dtm, log_transform=True)\n",
    "        wbe.write_raster(minic*scale, tmp_minic_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate minic--- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_minic_file)\n",
    "\n",
    "\n",
    "        # Maxic\n",
    "        tmp_maxic_file=input_file.replace('dtm','maxic')\n",
    "        scale=p_table[p_table['parameters']=='maxic'].multiplier.iloc[0]\n",
    "\n",
    "        start_time = time.time()\n",
    "        maxic = wbe.maximal_curvature(dtm, log_transform=True)\n",
    "        wbe.write_raster(maxic*scale, tmp_maxic_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        ttprint(f\"{cont} calculate maxic--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_maxic_file)\n",
    "\n",
    "        # Openness\n",
    "        tmp_pos_file=input_file.replace('dtm','pos.openness')\n",
    "        tmp_neg_file=input_file.replace('dtm','neg.openness')\n",
    "        start_time = time.time()\n",
    "        pos,neg = wbe.openness(dtm,dist=3)\n",
    "        scale=p_table[p_table['parameters']=='pos.openness'].multiplier.iloc[0]\n",
    "        wbe.write_raster(pos*scale, tmp_pos_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "        scale=p_table[p_table['parameters']=='neg.openness'].multiplier.iloc[0]\n",
    "\n",
    "        wbe.write_raster(neg*scale, tmp_neg_file, compress=True)#, compress=False) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "\n",
    "        ttprint(f\"{cont} calculate openness--- %s seconds ---\" % (time.time() - start_time))    \n",
    "        file_list.append(tmp_pos_file)\n",
    "        file_list.append(tmp_neg_file)\n",
    "\n",
    "\n",
    "        # profile curve\n",
    "        start_time = time.time()\n",
    "        tmp_procurv_file=input_file.replace('dtm','pro.curv')\n",
    "        scale=p_table[p_table['parameters']=='pro.curv'].multiplier.iloc[0]\n",
    "        procurv = wbe.profile_curvature(dtm, log_transform=True)\n",
    "\n",
    "        wbe.write_raster(procurv*scale, tmp_procurv_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "\n",
    "        ttprint(f\"{cont} profile curve --- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_procurv_file)\n",
    "\n",
    "\n",
    "        # shape index\n",
    "        start_time = time.time()\n",
    "        tmp_shpindx_file=input_file.replace('dtm','shpindx')\n",
    "        scale=p_table[p_table['parameters']=='shpindx'].multiplier.iloc[0]\n",
    "\n",
    "\n",
    "        shpindx=wbe.shape_index(dtm)\n",
    "        wbe.write_raster(shpindx*scale, tmp_shpindx_file, compress=True)\n",
    "        ttprint(f\"{cont} shape index --- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_shpindx_file)\n",
    "\n",
    "        # ring curvature\n",
    "        start_time = time.time()\n",
    "        tmp_ring_curv_file=input_file.replace('dtm','ring.curv')\n",
    "        scale=p_table[p_table['parameters']=='ring.curv'].multiplier.iloc[0]\n",
    "\n",
    "\n",
    "        ring_curv=wbe.ring_curvature(dtm, log_transform=True)\n",
    "        wbe.write_raster(ring_curv*scale, tmp_ring_curv_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "\n",
    "        ttprint(f\"{cont} ring curvature --- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_ring_curv_file)\n",
    "\n",
    "        # tangential curvatures\n",
    "        start_time = time.time()\n",
    "        tmp_tan_curv_file=input_file.replace('dtm','tan.curv')\n",
    "        scale=p_table[p_table['parameters']=='tan.curv'].multiplier.iloc[0]\n",
    "\n",
    "        tan_curv=wbe.tangential_curvature(dtm, log_transform=True)\n",
    "        wbe.write_raster(tan_curv*scale, tmp_tan_curv_file, compress=True) # Compression is good, but it is a bit slower so here we won't use it.\n",
    "\n",
    "        ttprint(f\"{cont} tangential curvature --- %s seconds ---\" % (time.time() - start_time))\n",
    "        file_list.append(tmp_tan_curv_file)\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        def para_gdal_warp(file_path,cont,bbox,p_table,tmp_landmask_file):\n",
    "            file_name = file_path.split('/')[-1]\n",
    "            parameter = file_name.split('_')[0]\n",
    "            dtype=p_table[p_table['parameters']==parameter].new_data_type.iloc[0]\n",
    "            no_data=p_table[p_table['parameters']==parameter].no_data.iloc[0]\n",
    "\n",
    "            gdalcmd = f'gdalwarp -overwrite -ot {dtype} -tr {resolution} {resolution} -te {bbox} -co TILED=YES -co BIGTIFF=YES -co COMPRESS=DEFLATE -co ZLEVEL=9 -co BLOCKXSIZE=2048 -co BLOCKYSIZE=2048 -co NUM_THREADS=8 -co SPARSE_OK=TRUE'\n",
    "\n",
    "            file_name = parameter + '_edtm' + '_m' + f'_{resolution}m' + '_s' + '_20000101_20221231' + f'_{cont.lower()}'  + '_equi7' + '_v20241230' + '.tif'\n",
    "            out_path = f'{outdir}/{file_name}'  \n",
    "            tmp_out_path = f'{outdir}/tmp_{file_name}'\n",
    "            os.system(f'{gdalcmd} {file_path} {tmp_out_path}')\n",
    "            # landmasking\n",
    "            os.system(f'gdal_calc.py -A {tmp_out_path} -B {tmp_landmask_file} --overwrite --outfile={out_path} \\\n",
    "                        --calc=\"(B==100)*A + (B!=100)*{no_data}\" --type={dtype} --co=\"ZLEVEL=9\" --co=\"COMPRESS=DEFLATE\" \\\n",
    "                        --co=\"BLOCKXSIZE=2048\" --NoDataValue={no_data} --co=\"BLOCKYSIZE=2048\" \\\n",
    "                        --co=\"NUM_THREADS=8\" --co=\"SPARSE_OK=TRUE\"')\n",
    "            os.remove(file_path)\n",
    "            return out_path,file_name\n",
    "\n",
    "        args = [(i,cont,gdalwarp_bbox,p_table,tmp_landmask_file) for i in file_list]\n",
    "        for arg in args:\n",
    "            out_file,rn_file=para_gdal_warp(arg[0],arg[1],arg[2],arg[3],arg[4])\n",
    "            s3_path = f\"v4/{cont}/{rn_file}\"\n",
    "            client.fput_object(s3_config['bucket'], s3_path, out_file)\n",
    "            os.remove(out_file)\n",
    "            ttprint(f'http://{ip}/tmp-global-geomorpho/{s3_path} on S3')\n",
    "        #os.remove(rn_file)\n",
    "        os.system(f'rm -r {tmp_outdir}/*')\n",
    "        ttprint(f\"{cont} crop and save to local--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "Parallel(n_jobs=6)(delayed(worker_cont)(i,p_table) for i in conts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
